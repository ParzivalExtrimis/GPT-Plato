{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "lr = 3e-4\n",
    "n_embbed = 384\n",
    "block_s = 128\n",
    "batch_s = 64\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "dropout_r = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve dataset chunks and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'datasets\\sam_harris_podcast_transcripts'\n",
    "pdf_files = glob.glob(os.path.join(directory_path, '*.pdf')) # get all dataset chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars:  1596906\n"
     ]
    }
   ],
   "source": [
    "#get the total size of dataset\n",
    "text = ''\n",
    "for pdf_path in pdf_files:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = reader.pages\n",
    "\n",
    "    # extracting text from page\n",
    "    for page in pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "print('Chars: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  ['\\n', ' ', '\"', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', 'é', '–']\n",
      "Vocab size:  82\n"
     ]
    }
   ],
   "source": [
    "#make vocab\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_s = len(vocab)\n",
    "\n",
    "print('Vocab: ', vocab)\n",
    "print('Vocab size: ', vocab_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder/ Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make encoder/ decoder\n",
    "#   make stoi, itos dicts to hold translations\n",
    "\n",
    "itos = {i : s for i, s in zip(range(vocab_s), vocab)}\n",
    "stoi = {s : i for i, s in itos.items()}\n",
    "\n",
    "def encode(in_str):\n",
    "    return [stoi[c] for c in in_str]\n",
    "\n",
    "def decode(in_int_list):\n",
    "    return [itos[x] for x in in_int_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277524, 159692, 159690)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_n = 0.8 # 80% of the dataset used in training.\n",
    "val_split_n = 0.1 # 10% of the dataset used in validation.\n",
    "test_split_n = 0.1 # 10% of the dataset used in testing.\n",
    "\n",
    "# encode text (data set) -> data\n",
    "data = encode(text)\n",
    "\n",
    "train_split = data[: int(train_split_n * len(data))]\n",
    "val_split = data[int(train_split_n * len(data)) : int(-val_split_n * len(data))]\n",
    "test_split = data[: int(test_split_n * len(data))]\n",
    "\n",
    "len(train_split), len(val_split), len(test_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Batch contruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "tensor([[ 1, 54, 57,  ..., 57, 65, 61],\n",
      "        [71,  1, 67,  ...,  1, 64, 67],\n",
      "        [53, 55, 57,  ..., 55, 71,  1],\n",
      "        ...,\n",
      "        [ 1, 61, 71,  ..., 72, 60, 57],\n",
      "        [65, 68,  1,  ...,  7,  1, 75],\n",
      "        [ 1, 58, 61,  ..., 57, 55, 67]], device='cuda:0')\n",
      "torch.Size([64, 128])\n",
      "tensor([[54, 57,  1,  ..., 65, 61, 71],\n",
      "        [ 1, 67, 73,  ..., 64, 67, 72],\n",
      "        [55, 57,  1,  ..., 71,  1,  1],\n",
      "        ...,\n",
      "        [61, 71, 71,  ..., 60, 57,  1],\n",
      "        [68,  1,  1,  ...,  1, 75, 60],\n",
      "        [58, 61, 66,  ..., 55, 67, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# make a func to take in data in splits and return Xs and Ys\n",
    "def build_batch(split):\n",
    "    #determine split type and use relevant set\n",
    "    match split:\n",
    "        case 'train':\n",
    "            dta = train_split\n",
    "        case 'val':\n",
    "            dta = val_split\n",
    "        case 'test':\n",
    "            dta = test_split\n",
    "    # sample block size examples at random from set\n",
    "    ix = torch.randint(0, len(dta) - block_s, (batch_s, ))\n",
    "    xs = torch.stack([torch.tensor(dta[i : i + block_s]) for i in ix])\n",
    "    ys = torch.stack([torch.tensor(dta[i + 1 : i + block_s + 1]) for i in ix])\n",
    "    xs, ys = xs.to(device), ys.to(device)\n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = build_batch('train')\n",
    "\n",
    "print(xs.shape)\n",
    "print(xs)\n",
    "print(ys.shape)\n",
    "print(ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, head_s):\n",
    "        super().__init__()\n",
    "        self.head_s = head_s\n",
    "\n",
    "        #initialize K, Q, V, tril\n",
    "        self.key = torch.nn.Linear(n_embbed, head_s, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embbed, head_s, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embbed, head_s, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_s, block_s)))\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        k = self.key(x) # (B, T, E) @ (E, H) -> (B, T, H)\n",
    "        q = self.query(x) # (B, T, H)\n",
    "\n",
    "        #get attention scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_s**-0.5) # (B, T, T) # /sqrt(head_S) to normalize the initalizations of heads\n",
    "        wei = torch.masked_fill(wei, self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)# section the matrix such that nodes can not see their children \n",
    "        wei = torch.softmax(wei, dim=1)# (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        #weighed aggrigation of the values\n",
    "        v = self.value(x) # ( B, T, E ) @ ( E, H ) -> (B, T, H)\n",
    "        out = wei @ v # ( B, T, T ) @ ( B, T, H ) -> ( B, T, H )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, size):\n",
    "        super().__init__()\n",
    "\n",
    "        #initalize each head as a module list\n",
    "        self.heads = torch.nn.ModuleList(AttentionHead(size) for _ in range(n_heads))\n",
    "        self.proj = torch.nn.Linear(n_embbed, n_embbed) # allows for the output to fork back into residual pathway\n",
    "        self.dropout = torch.nn.Dropout(dropout_r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #call each head in list sequentially and concat the outputs\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1) # h(B, T, H)\n",
    "        x = self.proj(x)\n",
    "        out = self.dropout(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        #init a sequential linear-> non-linear MLP\n",
    "        hidden_s = size * 4 # increases compute dimension of ffwd\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(size, hidden_s),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_s, size),# Projection: allows for the output to fork back into residual pathway\n",
    "            torch.nn.Dropout(dropout_r)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Container for MultiHead and FFwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, n_embbed, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        #initalize both multihead and ffwds\n",
    "        head_s = n_embbed // n_heads\n",
    "        self.mh = MultiheadAttention(n_heads, size=head_s)\n",
    "        self.ffwd = FeedForward(n_embbed)\n",
    "        self.attention_layer_norm = torch.nn.LayerNorm(n_embbed)\n",
    "        self.ffwd_layer_norm = torch.nn.LayerNorm(n_embbed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mh(self.attention_layer_norm(x))\n",
    "        x = x + self.ffwd(self.ffwd_layer_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        #initialize emb tables\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_s, n_embbed) # (vocab_s, embedding_dim_s)\n",
    "        self.pos_embedding_table = torch.nn.Embedding(block_s, n_embbed)\n",
    "\n",
    "        #layers\n",
    "        self.blocks = torch.nn.Sequential(*[Block(n_embbed, n_heads=n_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = torch.nn.LayerNorm(n_embbed) # final layer norm\n",
    "        self.lm_head = torch.nn.Linear(n_embbed, vocab_s)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embbeds = self.token_embedding_table(idx) # (B, T, E) { E = embedding_dim_s}\n",
    "        pos_embbeds = self.pos_embedding_table(torch.arange(0, T, device=device)) # (T, E)\n",
    "        embbeds = token_embbeds + pos_embbeds # (B, T, E)\n",
    "        x = self.blocks(embbeds) # (B, T, E)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, C) \n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        \n",
    "        # during generation targets, loss are not req\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, C) # (BT, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_gens):\n",
    "        for _ in range(max_gens):\n",
    "            #get logits from forward pass use the last character\n",
    "            #pass context_length(idx) i.e len <= block_size { position embedding in fwd only defined upto block_s}\n",
    "            context = idx[:, -block_s: ]\n",
    "            logits, _ = self(context) # (B, T, C) (b, t) (b, t, C)\n",
    "            logits = logits[:, -1, :] # (b, C)\n",
    "            #softmax logits to get probabilities \n",
    "            probs = torch.nn.functional.softmax(logits, dim=1) # (b, C)\n",
    "            #use probs to sample from multinomial and append to the idx\n",
    "            x = torch.multinomial(probs, num_samples=1, replacement=True) # (b, 1)\n",
    "            idx = torch.cat((idx, x), dim=1)  # # (b, C + 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 82])\n",
      "tensor(4.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "10.75285 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "model.to(device)\n",
    "logits, loss = model(xs, ys)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup optimizer\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval loss function\n",
    "@torch.no_grad()\n",
    "def eval_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    splits = ['train', 'val']\n",
    "    for split in splits:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = build_batch(split)\n",
    "            _ , loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss: 4.652235984802246, Validation Loss: 4.653196334838867\n",
      "500 Train Loss: 0.09002933651208878, Validation Loss: 0.08335676789283752\n",
      "1000 Train Loss: 0.025867408141493797, Validation Loss: 0.02376963384449482\n",
      "1500 Train Loss: 0.01910800114274025, Validation Loss: 0.018208393827080727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m#backward\u001b[39;00m\n\u001b[0;32m     15\u001b[0m optim\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     18\u001b[0m \u001b[39m#optimize\u001b[39;00m\n\u001b[0;32m     19\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\aryan\\.conda\\envs\\ai\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aryan\\.conda\\envs\\ai\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    #display loss\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = eval_loss()\n",
    "        print(f'{iter} Train Loss: {losses[\"train\"]}, Validation Loss: {losses[\"val\"]}')\n",
    "\n",
    "    #get batch from batch sampler\n",
    "    x, y = build_batch('train')\n",
    "    \n",
    "    #foward pass\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    #backward\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    #optimize\n",
    "    optim.step()\n",
    "    \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Do\n",
      "[0:79:ffffffffffffffffffffffffffra/fffffffmm-Mmm   i boi i uaazPAA3GPazzzz'1iiiicizzzv   Duccui70000-LAAd8H66uly e lthe  intesens  in  culls there  cenvectoerGetone ste  dre . Tcke  scth-  Oia  yop\n"
     ]
    }
   ],
   "source": [
    "#sample generator\n",
    "context = torch.zeros((1, 1), dtype=torch.int32, device=device)\n",
    "idx = model.generate(context, 200).squeeze(dim=0).tolist()\n",
    "out = ''.join(decode(idx))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 53, 65, 1, 31, 53, 70, 70, 61, 71]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text\n",
    "encode('Sam Harris')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
